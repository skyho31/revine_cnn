{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vgg4_ver1_0.01_0.0005.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/skyho31/revine_cnn/blob/master/vgg4_0_01_0_0005.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "cup2aTskvY5C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "outputId": "97da0b12-a062-4ccb-9da9-5a9c19550b22"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install cupy-cuda80 chainer"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libcusparse8.0 is already the newest version (8.0.61-1).\n",
            "libnvrtc8.0 is already the newest version (8.0.61-1).\n",
            "libnvtoolsext1 is already the newest version (8.0.61-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
            "Requirement already satisfied: cupy-cuda80 in /usr/local/lib/python3.6/dist-packages (4.1.0)\n",
            "Requirement already satisfied: chainer in /usr/local/lib/python3.6/dist-packages (4.1.0)\n",
            "Requirement already satisfied: fastrlock>=0.3 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (0.3)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.14.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from cupy-cuda80) (1.11.0)\n",
            "Requirement already satisfied: protobuf>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from chainer) (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from chainer) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.0.0->chainer) (39.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HefS85hHS0rW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1 - import module"
      ]
    },
    {
      "metadata": {
        "id": "uXQlGlNFN9Q-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "9f05db14-39a6-412d-aa35-ab033a0b32c9"
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import pickle\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import sys"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqMlQKdmS5If",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2 - define functions"
      ]
    },
    {
      "metadata": {
        "id": "qZ4jjk_8Os3y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "19c40be6-a0c8-4533-a23d-f8767cb75ef5"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# define def\n",
        "def identify_function(x):\n",
        "    return x\n",
        "\n",
        "def step_function(x):\n",
        "    return cp.array(x > 0, dtype=cp.int)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + cp.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def relu(x):\n",
        "    return cp.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = cp.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - cp.max(x, axis=0)\n",
        "        y = cp.exp(x) / cp.sum(cp.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - cp.max(x) # 오버플로 대책\n",
        "    return cp.exp(x) / cp.sum(cp.exp(x))\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * cp.sum((y-t)**2)\n",
        "\n",
        "# def cross_entropy_error(y, t):\n",
        "#     delta = 1e-7\n",
        "#     return -1 * cp.sum(t * cp.log(y + delta))\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -cp.sum(cp.log(y[cp.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "    \n",
        "def softmax_loss(x,t):\n",
        "    y = softmax(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "def im2col(icput_data, filter_h, filter_w, stride=1, pad=0):\n",
        "\n",
        "    N, C, H, W = icput_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = cp.pad(icput_data, [(0,0), (0,0), (pad, pad), (pad,pad)], 'constant')\n",
        "    col = cp.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "def col2im(col, icput_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = icput_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride+1\n",
        "    out_w = (W + 2*pad - filter_w)//stride+1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0,3,4,5,1,2)\n",
        "\n",
        "    img = cp.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride -1))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] = col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4\n",
        "    grad = cp.zeros_like(x)\n",
        "\n",
        "    it = cp.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRu1pxJWS8V7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3 - define optimizer\n",
        "adam"
      ]
    },
    {
      "metadata": {
        "id": "1CtgecYGO1_V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "71698069-ec6e-492d-8963-ec26d155d157"
      },
      "cell_type": "code",
      "source": [
        "# define optimizer\n",
        "class Adam:\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "\n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = cp.zeros_like(val)\n",
        "                self.v[key] = cp.zeros_like(val)\n",
        "\n",
        "        self.iter += 1\n",
        "        lr_t = self.lr * cp.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
        "\n",
        "        for key in params.keys():\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "\n",
        "            params[key] -= lr_t * self.m[key] / (cp.sqrt(self.v[key]) + 1e-8)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gg35Xeq_Thpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4 - define trainer"
      ]
    },
    {
      "metadata": {
        "id": "ovFL_nOlO6Z9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "77d11f09-5d55-4d74-ffa9-7797c370620c"
      },
      "cell_type": "code",
      "source": [
        "# define trainer\n",
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='adam', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "        \n",
        "        # custom\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def train_step(self, i, max_iter):\n",
        "#         start_time_each = time.time()\n",
        "           \n",
        "        batch_mask = cp.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "#         if self.verbose:\n",
        "#             end_time_each = time.time() - start_time_each\n",
        "#             infoStr = '=== max_iter:' + str(max_iter) + ' / ' +str(round(i/max_iter*100, 2)) + '%' + '       '  + 'train loss:' + str(loss) +'         time:' + str(int(end_time_each)) + ' seconds' \n",
        "#             print(infoStr)\n",
        "          \n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:  \n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample, self.batch_size)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample, self.batch_size)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose:\n",
        "              end_time = time.time() - self.start_time\n",
        "              print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + ', train loss:' + str(loss) + \", time :\" + str(int(end_time)) + ' seconds  ===')\n",
        "                \n",
        "              self.start_time = time.time()\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step(i, self.max_iter)\n",
        "         \n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test, self.batch_size)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "an2VvBQLTmsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 5 - define layer\n",
        "relu, convolution, pooling, affine"
      ]
    },
    {
      "metadata": {
        "id": "rESey8s6PCJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "a9b0820b-8f1b-43e8-de45-233392dad698"
      },
      "cell_type": "code",
      "source": [
        "# define layer\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "      \n",
        "class LeakyRelu:\n",
        "    def __init__(self, alpha=0.3):\n",
        "        self.mask = None\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = out[self.mask] * self.alpha\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = dout[self.mask] * self.alpha\n",
        "        dx = dout\n",
        "        \n",
        "        return dx\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape # 텐서 대책이란?\n",
        "        x = x.reshape(x.shape[0], -1) # reshape에서 -1의 의미는?\n",
        "        self.x = x\n",
        "\n",
        "        out = cp.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = cp.dot(dout, self.W.T)\n",
        "        self.dW = cp.dot(self.x.T, dout)\n",
        "        self.db = cp.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape) #입력 데이터 모양 변경 이유? 텐서 대응 how?\n",
        "        return dx\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[cp.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # for backward\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = cp.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "        \n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = cp.sum(dout, axis=0)\n",
        "        self.dW = cp.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = cp.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = cp.argmax(col, axis=1)\n",
        "        out = cp.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = cp.zeros((dout.size, pool_size))\n",
        "        dmax[cp.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "      \n",
        "class Dropout:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1207.0580\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout_ratio=0.5):\n",
        "        self.dropout_ratio = dropout_ratio\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        if train_flg:\n",
        "            self.mask = cp.random.rand(*x.shape) > self.dropout_ratio\n",
        "            return x * self.mask\n",
        "        else:\n",
        "            return x * (1.0 - self.dropout_ratio)\n",
        "\n",
        "    def backward(self, dout):\n",
        "        return dout * self.mask\n",
        "    \n",
        "class BatchNormalization:\n",
        "    \"\"\"\n",
        "    http://arxiv.org/abs/1502.03167\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None):\n",
        "        self.gamma = gamma\n",
        "        self.beta = beta\n",
        "        self.momentum = momentum\n",
        "        self.icput_shape = None # 합성곱 계층은 4차원, 완전연결 계층은 2차원  \n",
        "\n",
        "        # 시험할 때 사용할 평균과 분산\n",
        "        self.running_mean = running_mean\n",
        "        self.running_var = running_var  \n",
        "        \n",
        "        # backward 시에 사용할 중간 데이터\n",
        "        self.batch_size = None\n",
        "        self.xc = None\n",
        "        self.std = None\n",
        "        self.dgamma = None\n",
        "        self.dbeta = None\n",
        "\n",
        "    def forward(self, x, train_flg=True):\n",
        "        self.icput_shape = x.shape\n",
        "        if x.ndim != 2:\n",
        "            N, C, H, W = x.shape\n",
        "            x = x.reshape(N, -1)\n",
        "\n",
        "        out = self.__forward(x, train_flg)\n",
        "        \n",
        "        return out.reshape(*self.icput_shape)\n",
        "            \n",
        "    def __forward(self, x, train_flg):\n",
        "        if self.running_mean is None:\n",
        "            N, D = x.shape\n",
        "            self.running_mean = cp.zeros(D)\n",
        "            self.running_var = cp.zeros(D)\n",
        "                        \n",
        "        if train_flg:\n",
        "            mu = x.mean(axis=0)\n",
        "            xc = x - mu\n",
        "            var = cp.mean(xc**2, axis=0)\n",
        "            std = cp.sqrt(var + 10e-7)\n",
        "            xn = xc / std\n",
        "            \n",
        "            self.batch_size = x.shape[0]\n",
        "            self.xc = xc\n",
        "            self.xn = xn\n",
        "            self.std = std\n",
        "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
        "            self.running_var = self.momentum * self.running_var + (1-self.momentum) * var            \n",
        "        else:\n",
        "            xc = x - self.running_mean\n",
        "            xn = xc / ((cp.sqrt(self.running_var + 10e-7)))\n",
        "            \n",
        "        out = self.gamma * xn + self.beta \n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        if dout.ndim != 2:\n",
        "            N, C, H, W = dout.shape\n",
        "            dout = dout.reshape(N, -1)\n",
        "\n",
        "        dx = self.__backward(dout)\n",
        "\n",
        "        dx = dx.reshape(*self.icput_shape)\n",
        "        return dx\n",
        "\n",
        "    def __backward(self, dout):\n",
        "        dbeta = dout.sum(axis=0)\n",
        "        dgamma = cp.sum(self.xn * dout, axis=0)\n",
        "        dxn = self.gamma * dout\n",
        "        dxc = dxn / self.std\n",
        "        dstd = -cp.sum((dxn * self.xc) / (self.std * self.std), axis=0)\n",
        "        dvar = 0.5 * dstd / self.std\n",
        "        dxc += (2.0 / self.batch_size) * self.xc * dvar\n",
        "        dmu = cp.sum(dxc, axis=0)\n",
        "        dx = dxc - dmu / self.batch_size\n",
        "        \n",
        "        self.dgamma = dgamma\n",
        "        self.dbeta = dbeta\n",
        "        \n",
        "        return dx"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rt_8gRBcS_hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 6 - define PPNet"
      ]
    },
    {
      "metadata": {
        "id": "_8tQbHbwPHfI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "b0d82426-5190-454b-fb6f-6d3c3b164a4c"
      },
      "cell_type": "code",
      "source": [
        "class PPNet:\n",
        "    \"\"\"\n",
        "      conv - relu - conv - relu - batch - dropout\n",
        "      conv - relu - conv - relu - pooling - dropout\n",
        "      conv - relu - conv - relu - batch - dropout\n",
        "      conv - relu - conv - relu - conv - relu - pooling - \n",
        "      affine - leakyrelu - batch - dropout -\n",
        "      affine - leakyrelu - batch - dropout -\n",
        "      affine - softmax\n",
        "    \"\"\"\n",
        "  \n",
        "  \n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param_1={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_5={'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6={'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_7={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_8={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_9={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=256, output_size=10, weight_init_std=0.01, \n",
        "                 use_dropout=True, use_batchnorm=True):\n",
        "      \n",
        "        # 가중치 초기화\n",
        "        self.use_dropout = use_dropout\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        \n",
        "        conv_output_1 = (input_dim[1] - conv_param_1['filter_size'] + 2*conv_param_1['pad']) / conv_param_1['stride'] + 1\n",
        "        conv_output_2 = (conv_output_1 - conv_param_2['filter_size'] + 2*conv_param_2['pad']) / conv_param_2['stride'] + 1\n",
        "        conv_output_3 = (conv_output_2 - conv_param_3['filter_size'] + 2*conv_param_3['pad']) / conv_param_3['stride'] + 1\n",
        "        conv_output_4 = (conv_output_3 - conv_param_4['filter_size'] + 2*conv_param_4['pad']) / conv_param_4['stride'] + 1\n",
        "        pool_output_1 = conv_output_4 / 2\n",
        "        \n",
        "        conv_output_5 = (pool_output_1 - conv_param_5['filter_size'] + 2*conv_param_5['pad']) / conv_param_5['stride'] + 1\n",
        "        conv_output_6 = (conv_output_5 - conv_param_6['filter_size'] + 2*conv_param_6['pad']) / conv_param_6['stride'] + 1\n",
        "        conv_output_7 = (conv_output_6 - conv_param_7['filter_size'] + 2*conv_param_7['pad']) / conv_param_7['stride'] + 1\n",
        "        conv_output_8 = (conv_output_7 - conv_param_8['filter_size'] + 2*conv_param_8['pad']) / conv_param_8['stride'] + 1\n",
        "        conv_output_9 = (conv_output_8 - conv_param_9['filter_size'] + 2*conv_param_9['pad']) / conv_param_9['stride'] + 1\n",
        "        \n",
        "        pool_output_2 = conv_output_9 / 2\n",
        "                \n",
        "        pre_node_nums = [\n",
        "            input_dim[0] * conv_param_1['filter_size'] * conv_param_1['filter_size'],\n",
        "            conv_param_1['filter_num'] * conv_param_2['filter_size'] * conv_param_2['filter_size'],\n",
        "            conv_param_2['filter_num'] * conv_param_3['filter_size'] * conv_param_3['filter_size'],\n",
        "            conv_param_3['filter_num'] * conv_param_4['filter_size'] * conv_param_4['filter_size'],\n",
        "            conv_param_4['filter_num'] * conv_param_5['filter_size'] * conv_param_5['filter_size'],\n",
        "            conv_param_5['filter_num'] * conv_param_6['filter_size'] * conv_param_6['filter_size'],\n",
        "            conv_param_6['filter_num'] * conv_param_7['filter_size'] * conv_param_7['filter_size'],\n",
        "            conv_param_7['filter_num'] * conv_param_8['filter_size'] * conv_param_8['filter_size'],\n",
        "            conv_param_8['filter_num'] * conv_param_9['filter_size'] * conv_param_9['filter_size'],\n",
        "            conv_param_9['filter_num'] * int(pool_output_2) * int(pool_output_2),\n",
        "            hidden_size,\n",
        "            hidden_size\n",
        "        ]\n",
        "        \n",
        "        if weight_init_std == 'he':\n",
        "          weight_init_scales = cp.sqrt(2.0 / pre_node_nums) # he 초기값\n",
        "        else:\n",
        "          weight_init_scales = cp.full(pre_node_nums[0], weight_init_std)\n",
        "          \n",
        "                \n",
        "        self.params = {}\n",
        "        pre_channel_num = input_dim[0]\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6, conv_param_7, conv_param_8, conv_param_9]):\n",
        "          if weight_init_std == 'he':\n",
        "            self.params['W' + str(idx+1)] = weight_init_scales[idx] * cp.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "          else:\n",
        "            self.params['W' + str(idx+1)] = weight_init_std * cp.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\n",
        "  \n",
        "          self.params['b' + str(idx+1)] = cp.zeros(conv_param['filter_num'])\n",
        "          pre_channel_num = conv_param['filter_num']\n",
        "        \n",
        "        self.params['gamma1'] = cp.ones(int(conv_output_2)**2 * conv_param_2['filter_num'])\n",
        "        self.params['beta1'] = cp.zeros(int(conv_output_2)**2 * conv_param_2['filter_num'])\n",
        "        self.params['gamma2'] = cp.ones(int(conv_output_6)**2 * conv_param_6['filter_num'])\n",
        "        self.params['beta2'] = cp.zeros(int(conv_output_6)**2 * conv_param_6['filter_num'])\n",
        "        self.params['gamma3'] = cp.ones(hidden_size)\n",
        "        self.params['beta3'] = cp.zeros(hidden_size)\n",
        "        self.params['gamma4'] = cp.ones(hidden_size)\n",
        "        self.params['beta4'] = cp.zeros(hidden_size)\n",
        "\n",
        "        self.params['W10'] = weight_init_scales[4] * cp.random.randn(pre_node_nums[9], hidden_size)\n",
        "        self.params['b10'] = cp.zeros(hidden_size)\n",
        "        self.params['W11'] = weight_init_scales[5] * cp.random.randn(hidden_size, hidden_size)\n",
        "        self.params['b11'] = cp.zeros(hidden_size)\n",
        "        self.params['W12'] = weight_init_scales[6] * cp.random.randn(hidden_size, output_size)\n",
        "        self.params['b12'] = cp.zeros(output_size)\n",
        "\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = []\n",
        "\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], conv_param_1['stride'], conv_param_1['pad'])) #0\n",
        "        self.layers.append(Relu()) #1\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], conv_param_2['stride'], conv_param_2['pad'])) #2\n",
        "        self.layers.append(Relu()) #3\n",
        "        self.layers.append(BatchNormalization(self.params['gamma1'], self.params['beta1'])) #4\n",
        "        self.layers.append(Dropout(0.25)) #5\n",
        "        \n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], conv_param_3['stride'], conv_param_3['pad'])) #6\n",
        "        self.layers.append(Relu()) #7\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'], conv_param_4['stride'], conv_param_4['pad'])) #8\n",
        "        self.layers.append(Relu()) #9\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2)) #10\n",
        "        self.layers.append(Dropout(0.25)) #11\n",
        "        \n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'], conv_param_3['stride'], conv_param_3['pad'])) #12\n",
        "        self.layers.append(Relu()) #13\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'], conv_param_4['stride'], conv_param_4['pad'])) #14\n",
        "        self.layers.append(Relu()) #15\n",
        "        self.layers.append(BatchNormalization(self.params['gamma2'], self.params['beta2'])) #16\n",
        "        self.layers.append(Dropout(0.25)) #17\n",
        "        \n",
        "        self.layers.append(Convolution(self.params['W7'], self.params['b7'], conv_param_3['stride'], conv_param_3['pad'])) #18\n",
        "        self.layers.append(Relu()) #19\n",
        "        self.layers.append(Convolution(self.params['W8'], self.params['b8'], conv_param_4['stride'], conv_param_4['pad'])) #20\n",
        "        self.layers.append(Relu()) #21\n",
        "        self.layers.append(Convolution(self.params['W9'], self.params['b9'], conv_param_4['stride'], conv_param_4['pad'])) #22\n",
        "        self.layers.append(Relu()) #23\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2)) #24\n",
        "        \n",
        "        self.layers.append(Affine(self.params['W10'], self.params['b10'])) #25\n",
        "        self.layers.append(LeakyRelu()) # 26\n",
        "        self.layers.append(BatchNormalization(self.params['gamma3'], self.params['beta3'])) #27\n",
        "        self.layers.append(Dropout(0.5)) #28\n",
        "        \n",
        "        self.layers.append(Affine(self.params['W11'], self.params['b11'])) #29\n",
        "        self.layers.append(LeakyRelu()) #30\n",
        "        self.layers.append(BatchNormalization(self.params['gamma4'], self.params['beta4'])) #31\n",
        "        self.layers.append(Dropout(0.5)) #32\n",
        "        \n",
        "        self.layers.append(Affine(self.params['W12'], self.params['b12'])) # 33\n",
        "        self.last_layer = SoftmaxWithLoss() #34\n",
        "\n",
        "\n",
        "    def predict(self, x, train_flg=True):\n",
        "        for layer in self.layers:\n",
        "          if isinstance(layer, Dropout) | isinstance(layer, BatchNormalization):\n",
        "            x = layer.forward(x, train_flg)\n",
        "          else:\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t, train_flg=True):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x, train_flg)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = cp.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx, train_flg=False)\n",
        "            y = cp.argmax(y, axis=1)\n",
        "            acc += cp.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, '1', 3, 4, 5, 6, 7, 8, 9, 10, '2', 11, '3', 12):\n",
        "            if type(idx) == int:\n",
        "                grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "                grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "            else:\n",
        "                grads['gamma' + idx] = numerical_gradient(loss_w, self.params['gamma' + idx])\n",
        "                grads['beta' + idx] = numerical_gradient(loss_w, self.params['beta' + idx])\n",
        "        \n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "        \n",
        "        tmp_layers = self.layers.copy()\n",
        "        tmp_layers.reverse()\n",
        "        for layer in tmp_layers:\n",
        "          dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        tmpTuple = (0, 2, 6, 8, 12, 14, 18, 20, 22, 25, 29, 33)\n",
        "        batchTuple = (4, 16, 27, 31)\n",
        "                \n",
        "        for i, layer_idx in enumerate(tmpTuple):\n",
        "          grads['W' + str(i+1)] = self.layers[layer_idx].dW\n",
        "          grads['b' + str(i+1)] = self.layers[layer_idx].db\n",
        "              \n",
        "        for i, layer_idx in enumerate(batchTuple):\n",
        "          grads['gamma' + str(i+1)] = self.layers[layer_idx].dgamma\n",
        "          grads['beta' + str(i+1)] = self.layers[layer_idx].dbeta\n",
        "\n",
        "        return grads\n",
        "\n",
        "\n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "      \n",
        "        for i, key in enumerate((0, 2, 6, 8, 12, 14, 18, 20, 22, 25, 29, 33)):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n",
        "            \n",
        "        for i, key in enumerate((4, 16, 27, 31)):\n",
        "            self.layers[key].gamma = self.params['W' + str(i+1)]\n",
        "            self.layers[key].beta = self.params['b' + str(i+1)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8T4cJZqXTKsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 7 - execute code"
      ]
    },
    {
      "metadata": {
        "id": "4lMbW7VjgHsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 989
        },
        "outputId": "847eac3c-23ba-4640-cb2b-21b73de4bbfd"
      },
      "cell_type": "code",
      "source": [
        "# randomSeed\n",
        "cp.random.seed(9)\n",
        "\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train=cp.expand_dims(x_train,axis=1)\n",
        "x_test=cp.expand_dims(x_test,axis=1)\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "# x_train, t_train = x_train[:500], t_train[:500]\n",
        "# x_test, t_test = x_test[:100], t_test[:100]\n",
        "\n",
        "# 0-1로 정규화\n",
        "x_train = (x_train.astype(cp.float32))/255\n",
        "x_test = (x_test.astype(cp.float32))/255\n",
        "\n",
        "# x_train -= 0.5\n",
        "# x_test -= 0.5\n",
        "\n",
        "# x_train *= 2.\n",
        "# x_test *= 2.\n",
        "\n",
        "x_train = cp.array(x_train)\n",
        "x_test = cp.array(x_test)\n",
        "t_train = cp.array(t_train)\n",
        "t_test = cp.array(t_test)\n",
        "\n",
        "max_epochs = 50\n",
        "\n",
        "batch_size = 200\n",
        "validation_step =  t_train.shape[0] // batch_size\n",
        "\n",
        "network = PPNet(input_dim=(1, 28, 28), \n",
        "                 conv_param_1={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_2={'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_3={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_4={'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_5={'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_6={'filter_num':128, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_7={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_8={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 conv_param_9={'filter_num':256, 'filter_size':3, 'pad':1, 'stride':1},\n",
        "                 hidden_size=512, output_size=10, weight_init_std=0.01)              \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=batch_size,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.0005},\n",
        "                  evaluate_sample_num_per_epoch=validation_step * batch_size)\n",
        "# 시간 체크\n",
        "start_time = time.time()\n",
        "trainer.train()\n",
        "\n",
        "# 체크 종료\n",
        "end_time = time.time() - start_time\n",
        "print(\"PPNet took for % seconds\" % end_time)\n",
        "\n",
        "# 매개변수 보관\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== epoch:1, train acc:0.16815, test acc:0.167, train loss:2.0588546160168057, time :177 seconds  ===\n",
            "=== epoch:2, train acc:0.8087, test acc:0.7965, train loss:0.5656254546019953, time :717 seconds  ===\n",
            "=== epoch:3, train acc:0.80175, test acc:0.7893, train loss:0.5635013000599122, time :717 seconds  ===\n",
            "=== epoch:4, train acc:0.8576333333333334, test acc:0.8445, train loss:0.5730680120512588, time :717 seconds  ===\n",
            "=== epoch:5, train acc:0.8308166666666666, test acc:0.8225, train loss:0.4247367577347637, time :717 seconds  ===\n",
            "=== epoch:6, train acc:0.85415, test acc:0.8432, train loss:0.4014417467332853, time :717 seconds  ===\n",
            "=== epoch:7, train acc:0.8636166666666667, test acc:0.8489, train loss:0.4276279709251536, time :717 seconds  ===\n",
            "=== epoch:8, train acc:0.8727166666666667, test acc:0.8589, train loss:0.4096717682297634, time :717 seconds  ===\n",
            "=== epoch:9, train acc:0.8618166666666667, test acc:0.8507, train loss:0.4103711452699369, time :717 seconds  ===\n",
            "=== epoch:10, train acc:0.88465, test acc:0.8691, train loss:0.3008832082351506, time :717 seconds  ===\n",
            "=== epoch:11, train acc:0.8770333333333333, test acc:0.8662, train loss:0.35664465934289524, time :717 seconds  ===\n",
            "=== epoch:12, train acc:0.8679333333333333, test acc:0.8596, train loss:0.31848204786162043, time :717 seconds  ===\n",
            "=== epoch:13, train acc:0.89645, test acc:0.8851, train loss:0.30065597790500626, time :717 seconds  ===\n",
            "=== epoch:14, train acc:0.9006666666666666, test acc:0.8847, train loss:0.3531155852340425, time :717 seconds  ===\n",
            "=== epoch:15, train acc:0.9053333333333333, test acc:0.8838, train loss:0.25592575040862325, time :717 seconds  ===\n",
            "=== epoch:16, train acc:0.90655, test acc:0.8899, train loss:0.33784283843719043, time :717 seconds  ===\n",
            "=== epoch:17, train acc:0.9106666666666666, test acc:0.8904, train loss:0.24915848524631257, time :717 seconds  ===\n",
            "=== epoch:18, train acc:0.9127333333333333, test acc:0.8897, train loss:0.3113035584423929, time :717 seconds  ===\n",
            "=== epoch:19, train acc:0.9132, test acc:0.8889, train loss:0.21303630635319298, time :717 seconds  ===\n",
            "=== epoch:20, train acc:0.91395, test acc:0.8896, train loss:0.2389931674195188, time :717 seconds  ===\n",
            "=== epoch:21, train acc:0.9166, test acc:0.8935, train loss:0.20338832381281022, time :717 seconds  ===\n",
            "=== epoch:22, train acc:0.9171333333333334, test acc:0.8941, train loss:0.28188428594171866, time :717 seconds  ===\n",
            "=== epoch:23, train acc:0.9236333333333333, test acc:0.8999, train loss:0.2385895225760273, time :717 seconds  ===\n",
            "=== epoch:24, train acc:0.924, test acc:0.8988, train loss:0.2325154665857449, time :717 seconds  ===\n",
            "=== epoch:25, train acc:0.9232666666666667, test acc:0.8967, train loss:0.23046704892430064, time :717 seconds  ===\n",
            "=== epoch:26, train acc:0.9190333333333334, test acc:0.8939, train loss:0.28612154337357054, time :717 seconds  ===\n",
            "=== epoch:27, train acc:0.9244333333333333, test acc:0.8999, train loss:0.21457320714485562, time :717 seconds  ===\n",
            "=== epoch:28, train acc:0.908, test acc:0.8877, train loss:0.2771052669091522, time :717 seconds  ===\n",
            "=== epoch:29, train acc:0.9079166666666667, test acc:0.8867, train loss:0.22944216706515874, time :717 seconds  ===\n",
            "=== epoch:30, train acc:0.9164, test acc:0.8955, train loss:0.3348150176718041, time :717 seconds  ===\n",
            "=== epoch:31, train acc:0.9220833333333334, test acc:0.8984, train loss:0.2151030542147034, time :717 seconds  ===\n",
            "=== epoch:32, train acc:0.9257, test acc:0.8966, train loss:0.1781065464674154, time :717 seconds  ===\n",
            "=== epoch:33, train acc:0.9175, test acc:0.8952, train loss:0.19434144036649037, time :717 seconds  ===\n",
            "=== epoch:34, train acc:0.9198666666666667, test acc:0.8975, train loss:0.1962482116666451, time :717 seconds  ===\n",
            "=== epoch:35, train acc:0.9220166666666667, test acc:0.8973, train loss:0.2114012792754383, time :717 seconds  ===\n",
            "=== epoch:36, train acc:0.9269166666666667, test acc:0.8978, train loss:0.2129983079209916, time :717 seconds  ===\n",
            "=== epoch:37, train acc:0.9319833333333334, test acc:0.9001, train loss:0.2319324271746617, time :717 seconds  ===\n",
            "=== epoch:38, train acc:0.9221833333333334, test acc:0.8944, train loss:0.22147575112265006, time :717 seconds  ===\n",
            "=== epoch:39, train acc:0.9261166666666667, test acc:0.8965, train loss:0.24541598611558996, time :717 seconds  ===\n",
            "=== epoch:40, train acc:0.9319166666666666, test acc:0.9034, train loss:0.20007543506180397, time :717 seconds  ===\n",
            "=== epoch:41, train acc:0.9374333333333333, test acc:0.9055, train loss:0.1477531251189186, time :717 seconds  ===\n",
            "=== epoch:42, train acc:0.9322666666666667, test acc:0.9004, train loss:0.18696699109334683, time :717 seconds  ===\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "=== epoch:43, train acc:0.9239, test acc:0.8922, train loss:0.22459409484857706, time :717 seconds  ===\n",
            "=== epoch:44, train acc:0.9336, test acc:0.8973, train loss:0.23345347169093608, time :717 seconds  ===\n",
            "=== epoch:45, train acc:0.9181333333333334, test acc:0.8922, train loss:0.17192516602466631, time :717 seconds  ===\n",
            "=== epoch:46, train acc:0.9231166666666667, test acc:0.8963, train loss:0.19709787514757962, time :717 seconds  ===\n",
            "=== epoch:47, train acc:0.9306333333333333, test acc:0.8988, train loss:0.1703225002025536, time :717 seconds  ===\n",
            "=== epoch:48, train acc:0.93725, test acc:0.9011, train loss:0.2009393228679209, time :717 seconds  ===\n",
            "=== epoch:49, train acc:0.9352666666666667, test acc:0.9002, train loss:0.26361224882376993, time :717 seconds  ===\n",
            "=== epoch:50, train acc:0.9418833333333333, test acc:0.9066, train loss:0.20974055277373854, time :717 seconds  ===\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9006\n",
            "PPNet took for 35903.88084411621econds\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZJ7kWjFYQ84o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "f428406b-54f6-4508-88a4-0a9e823ff5bd"
      },
      "cell_type": "code",
      "source": [
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAe8AAAFcCAYAAADh1zYWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8sGuQnnAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fe0b9b14358>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}