{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simpleConvnet_with_cuda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/skyho31/revine_cnn/blob/master/simpleConvnet_with_cuda.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "C1WahOek584i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Step 0 - Install Cupy"
      ]
    },
    {
      "metadata": {
        "id": "g09AZuzY58D7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt -y install libcusparse8.0 libnvrtc8.0 libnvtoolsext1\n",
        "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "!pip install cupy-cuda80"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HefS85hHS0rW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 1 - import module"
      ]
    },
    {
      "metadata": {
        "id": "uXQlGlNFN9Q-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import pickle\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqMlQKdmS5If",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 2 - define functions"
      ]
    },
    {
      "metadata": {
        "id": "qZ4jjk_8Os3y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# define def\n",
        "def identify_function(x):\n",
        "    return x\n",
        "\n",
        "def step_function(x):\n",
        "    return cp.array(x > 0, dtype=cp.int)\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + cp.exp(-x))\n",
        "\n",
        "def sigmoid_grad(x):\n",
        "    return (1.0 - sigmoid(x)) * sigmoid(x)\n",
        "\n",
        "def relu(x):\n",
        "    return cp.maximum(0, x)\n",
        "\n",
        "def relu_grad(x):\n",
        "    grad = cp.zeros(x)\n",
        "    grad[x>=0] = 1\n",
        "    return grad\n",
        "\n",
        "def softmax(x):\n",
        "    if x.ndim == 2:\n",
        "        x = x.T\n",
        "        x = x - cp.max(x, axis=0)\n",
        "        y = cp.exp(x) / cp.sum(cp.exp(x), axis=0)\n",
        "        return y.T \n",
        "\n",
        "    x = x - cp.max(x) # 오버플로 대책\n",
        "    return cp.exp(x) / cp.sum(cp.exp(x))\n",
        "\n",
        "def mean_squared_error(y, t):\n",
        "    return 0.5 * cp.sum((y-t)**2)\n",
        "\n",
        "# def cross_entropy_error(y, t):\n",
        "#     delta = 1e-7\n",
        "#     return -1 * cp.sum(t * cp.log(y + delta))\n",
        "def cross_entropy_error(y, t):\n",
        "    if y.ndim == 1:\n",
        "        t = t.reshape(1, t.size)\n",
        "        y = y.reshape(1, y.size)\n",
        "        \n",
        "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
        "    if t.size == y.size:\n",
        "        t = t.argmax(axis=1)\n",
        "             \n",
        "    batch_size = y.shape[0]\n",
        "    return -cp.sum(cp.log(y[cp.arange(batch_size), t] + 1e-7)) / batch_size\n",
        "    \n",
        "def softmax_loss(x,t):\n",
        "    y = softmax(x)\n",
        "    return cross_entropy_error(y, t)\n",
        "\n",
        "def im2col(icput_data, filter_h, filter_w, stride=1, pad=0):\n",
        "\n",
        "    N, C, H, W = icput_data.shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride + 1\n",
        "    out_w = (W + 2*pad - filter_w)//stride + 1\n",
        "\n",
        "    img = cp.pad(icput_data, [(0,0), (0,0), (pad, pad), (pad,pad)], 'constant')\n",
        "    col = cp.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
        "\n",
        "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\n",
        "    return col\n",
        "\n",
        "def col2im(col, icput_shape, filter_h, filter_w, stride=1, pad=0):\n",
        "    N, C, H, W = icput_shape\n",
        "    out_h = (H + 2*pad - filter_h)//stride+1\n",
        "    out_w = (W + 2*pad - filter_w)//stride+1\n",
        "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0,3,4,5,1,2)\n",
        "\n",
        "    img = cp.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride -1))\n",
        "\n",
        "    for y in range(filter_h):\n",
        "        y_max = y + stride*out_h\n",
        "        for x in range(filter_w):\n",
        "            x_max = x + stride*out_w\n",
        "            img[:, :, y:y_max:stride, x:x_max:stride] = col[:, :, y, x, :, :]\n",
        "\n",
        "    return img[:, :, pad:H + pad, pad:W + pad]\n",
        "\n",
        "def numerical_gradient(f, x):\n",
        "    h = 1e-4\n",
        "    grad = cp.zeros_like(x)\n",
        "\n",
        "    it = cp.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
        "    while not it.finished:\n",
        "        idx = it.multi_index\n",
        "        tmp_val = x[idx]\n",
        "        fxh1 = f(x)\n",
        "\n",
        "        x[idx] = tmp_val - h\n",
        "        fxh2 = f(x)\n",
        "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
        "\n",
        "        x[idx] = tmp_val\n",
        "        it.iternext()\n",
        "\n",
        "    return grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lRu1pxJWS8V7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 3 - define optimizer\n",
        "adam"
      ]
    },
    {
      "metadata": {
        "id": "1CtgecYGO1_V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "\n",
        "class SGD:\n",
        "\n",
        "    \"\"\"확률적 경사 하강법（Stochastic Gradient Descent）\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        for key in params.keys():\n",
        "            params[key] -= self.lr * grads[key] \n",
        "\n",
        "\n",
        "class Momentum:\n",
        "\n",
        "    \"\"\"모멘텀 SGD\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():                                \n",
        "                self.v[key] = cp.zeros_like(val)\n",
        "                \n",
        "        for key in params.keys():\n",
        "            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] \n",
        "            params[key] += self.v[key]\n",
        "\n",
        "\n",
        "class Nesterov:\n",
        "\n",
        "    \"\"\"Nesterov's Accelerated Gradient (http://arxiv.org/abs/1212.0901)\"\"\"\n",
        "    # NAG는 모멘텀에서 한 단계 발전한 방법이다. (http://newsight.tistory.com/224)\n",
        "    \n",
        "    def __init__(self, lr=0.01, momentum=0.9):\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.v is None:\n",
        "            self.v = {}\n",
        "            for key, val in params.items():\n",
        "                self.v[key] = cp.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.v[key] *= self.momentum\n",
        "            self.v[key] -= self.lr * grads[key]\n",
        "            params[key] += self.momentum * self.momentum * self.v[key]\n",
        "            params[key] -= (1 + self.momentum) * self.lr * grads[key]\n",
        "\n",
        "\n",
        "class AdaGrad:\n",
        "\n",
        "    \"\"\"AdaGrad\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01):\n",
        "        self.lr = lr\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = cp.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] += grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (cp.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class RMSprop:\n",
        "\n",
        "    \"\"\"RMSprop\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.01, decay_rate = 0.99):\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.h = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.h is None:\n",
        "            self.h = {}\n",
        "            for key, val in params.items():\n",
        "                self.h[key] = cp.zeros_like(val)\n",
        "            \n",
        "        for key in params.keys():\n",
        "            self.h[key] *= self.decay_rate\n",
        "            self.h[key] += (1 - self.decay_rate) * grads[key] * grads[key]\n",
        "            params[key] -= self.lr * grads[key] / (cp.sqrt(self.h[key]) + 1e-7)\n",
        "\n",
        "\n",
        "class Adam:\n",
        "\n",
        "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\n",
        "\n",
        "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.iter = 0\n",
        "        self.m = None\n",
        "        self.v = None\n",
        "        \n",
        "    def update(self, params, grads):\n",
        "        if self.m is None:\n",
        "            self.m, self.v = {}, {}\n",
        "            for key, val in params.items():\n",
        "                self.m[key] = cp.zeros_like(val)\n",
        "                self.v[key] = cp.zeros_like(val)\n",
        "        \n",
        "        self.iter += 1\n",
        "        lr_t  = self.lr * cp.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \n",
        "        \n",
        "        for key in params.keys():\n",
        "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\n",
        "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\n",
        "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\n",
        "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\n",
        "            \n",
        "            params[key] -= lr_t * self.m[key] / (cp.sqrt(self.v[key]) + 1e-7)\n",
        "            \n",
        "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\n",
        "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\n",
        "            #params[key] += self.lr * unbias_m / (cp.sqrt(unbisa_b) + 1e-7)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gg35Xeq_Thpj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 4 - define trainer"
      ]
    },
    {
      "metadata": {
        "id": "ovFL_nOlO6Z9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    \"\"\"신경망 훈련을 대신 해주는 클래스\n",
        "    \"\"\"\n",
        "    def __init__(self, network, x_train, t_train, x_test, t_test,\n",
        "                 epochs=20, mini_batch_size=100,\n",
        "                 optimizer='SGD', optimizer_param={'lr':0.01}, \n",
        "                 evaluate_sample_num_per_epoch=None, verbose=True):\n",
        "        self.network = network\n",
        "        self.verbose = verbose\n",
        "        self.x_train = x_train\n",
        "        self.t_train = t_train\n",
        "        self.x_test = x_test\n",
        "        self.t_test = t_test\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = mini_batch_size\n",
        "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\n",
        "\n",
        "        # optimzer\n",
        "        optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\n",
        "                                'adagrad':AdaGrad, 'rmsprpo':RMSprop, 'adam':Adam}\n",
        "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\n",
        "        \n",
        "        self.train_size = x_train.shape[0]\n",
        "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1)\n",
        "        self.max_iter = int(epochs * self.iter_per_epoch)\n",
        "        self.current_iter = 0\n",
        "        self.current_epoch = 0\n",
        "        \n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.test_acc_list = []\n",
        "        \n",
        "    def train_step(self):\n",
        "        batch_mask = cp.random.choice(self.train_size, self.batch_size)\n",
        "        x_batch = self.x_train[batch_mask]\n",
        "        t_batch = self.t_train[batch_mask]\n",
        "        \n",
        "        grads = self.network.gradient(x_batch, t_batch)\n",
        "        self.optimizer.update(self.network.params, grads)\n",
        "        \n",
        "        loss = self.network.loss(x_batch, t_batch)\n",
        "        self.train_loss_list.append(loss)\n",
        "#         if self.verbose: print(\"train loss:\" + str(loss))\n",
        "        \n",
        "        if self.current_iter % self.iter_per_epoch == 0:\n",
        "            self.current_epoch += 1\n",
        "            \n",
        "            x_train_sample, t_train_sample = self.x_train, self.t_train\n",
        "            x_test_sample, t_test_sample = self.x_test, self.t_test\n",
        "            if not self.evaluate_sample_num_per_epoch is None:\n",
        "                t = self.evaluate_sample_num_per_epoch\n",
        "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\n",
        "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\n",
        "                \n",
        "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\n",
        "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\n",
        "            self.train_acc_list.append(train_acc)\n",
        "            self.test_acc_list.append(test_acc)\n",
        "\n",
        "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\n",
        "        self.current_iter += 1\n",
        "\n",
        "    def train(self):\n",
        "        for i in range(self.max_iter):\n",
        "            self.train_step()\n",
        "\n",
        "        test_acc = self.network.accuracy(self.x_test, self.t_test)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(\"=============== Final Test Accuracy ===============\")\n",
        "            print(\"test acc:\" + str(test_acc))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "an2VvBQLTmsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# step 5 - define layer\n",
        "relu, convolution, pooling, affine"
      ]
    },
    {
      "metadata": {
        "id": "rESey8s6PCJC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define layer\n",
        "class Relu:\n",
        "    def __init__(self):\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.mask = (x <= 0)\n",
        "        out = x.copy()\n",
        "        out[self.mask] = 0\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout[self.mask] = 0\n",
        "        dx = dout\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Sigmoid:\n",
        "    def __init__(self):\n",
        "        self.out = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = sigmoid(x)\n",
        "        self.out = out\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = dout * (1.0 - self.out) * self.out\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Affine:\n",
        "    def __init__(self, W, b):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "\n",
        "        self.x = None\n",
        "        self.original_x_shape = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.original_x_shape = x.shape # 텐서 대책이란?\n",
        "        x = x.reshape(x.shape[0], -1) # reshape에서 -1의 의미는?\n",
        "        self.x = x\n",
        "\n",
        "        out = cp.dot(self.x, self.W) + self.b\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dx = cp.dot(dout, self.W.T)\n",
        "        self.dW = cp.dot(self.x.T, dout)\n",
        "        self.db = cp.sum(dout, axis=0)\n",
        "\n",
        "        dx = dx.reshape(*self.original_x_shape) #입력 데이터 모양 변경 이유? 텐서 대응 how?\n",
        "        return dx\n",
        "\n",
        "class SoftmaxWithLoss:\n",
        "    def __init__(self):\n",
        "        self.loss = None\n",
        "        self.y = None\n",
        "        self.t = None\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        self.t = t\n",
        "        self.y = softmax(x)\n",
        "        self.loss = cross_entropy_error(self.y, self.t)\n",
        "\n",
        "        return self.loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        batch_size = self.t.shape[0]\n",
        "        if self.t.size == self.y.size:\n",
        "            dx = (self.y - self.t) / batch_size\n",
        "        else:\n",
        "            dx = self.y.copy()\n",
        "            dx[cp.arange(batch_size), self.t] -= 1\n",
        "            dx = dx / batch_size\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Convolution:\n",
        "    def __init__(self, W, b, stride=1, pad=0):\n",
        "        self.W = W\n",
        "        self.b = b\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        # for backward\n",
        "        self.x = None\n",
        "        self.col = None\n",
        "        self.col_W = None\n",
        "\n",
        "        self.dW = None\n",
        "        self.db = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\n",
        "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\n",
        "\n",
        "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
        "        col_W = self.W.reshape(FN, -1).T\n",
        "\n",
        "        out = cp.dot(col, col_W) + self.b\n",
        "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
        "        \n",
        "        self.x = x\n",
        "        self.col = col\n",
        "        self.col_W = col_W\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        FN, C, FH, FW = self.W.shape\n",
        "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\n",
        "\n",
        "        self.db = cp.sum(dout, axis=0)\n",
        "        self.dW = cp.dot(self.col.T, dout)\n",
        "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\n",
        "\n",
        "        dcol = cp.dot(dout, self.col_W.T)\n",
        "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\n",
        "\n",
        "        return dx\n",
        "\n",
        "class Pooling:\n",
        "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
        "        self.pool_h = pool_h\n",
        "        self.pool_w = pool_w\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "\n",
        "        self.x = None\n",
        "        self.arg_max = None\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.shape\n",
        "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
        "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
        "\n",
        "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
        "\n",
        "        arg_max = cp.argmax(col, axis=1)\n",
        "        out = cp.max(col, axis=1)\n",
        "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
        "\n",
        "        self.x = x\n",
        "        self.arg_max = arg_max\n",
        "\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        dout = dout.transpose(0, 2, 3, 1)\n",
        "\n",
        "        pool_size = self.pool_h * self.pool_w\n",
        "        dmax = cp.zeros((dout.size, pool_size))\n",
        "        dmax[cp.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\n",
        "        dmax = dmax.reshape(dout.shape + (pool_size,))\n",
        "\n",
        "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\n",
        "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\n",
        "\n",
        "        return dx"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rt_8gRBcS_hF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 6 - define Convnet"
      ]
    },
    {
      "metadata": {
        "id": "_8tQbHbwPHfI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SimpleConvNet:\n",
        "    \"\"\"단순한 합성곱 신경망\n",
        "    \n",
        "    conv - relu - pool - affine - relu - affine - softmax\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_size : 입력 크기（MNIST의 경우엔 784）\n",
        "    hidden_size_list : 각 은닉층의 뉴런 수를 담은 리스트（e.g. [100, 100, 100]）\n",
        "    output_size : 출력 크기（MNIST의 경우엔 10）\n",
        "    activation : 활성화 함수 - 'relu' 혹은 'sigmoid'\n",
        "    weight_init_std : 가중치의 표준편차 지정（e.g. 0.01）\n",
        "        'relu'나 'he'로 지정하면 'He 초깃값'으로 설정\n",
        "        'sigmoid'나 'xavier'로 지정하면 'Xavier 초깃값'으로 설정\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=(1, 28, 28), \n",
        "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
        "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
        "        filter_num = conv_param['filter_num']\n",
        "        filter_size = conv_param['filter_size']\n",
        "        filter_pad = conv_param['pad']\n",
        "        filter_stride = conv_param['stride']\n",
        "        input_size = input_dim[1]\n",
        "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
        "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
        "\n",
        "        # 가중치 초기화\n",
        "        self.params = {}\n",
        "        self.params['W1'] = weight_init_std * \\\n",
        "                            cp.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
        "        self.params['b1'] = cp.zeros(filter_num)\n",
        "        self.params['W2'] = weight_init_std * \\\n",
        "                            cp.random.randn(pool_output_size, hidden_size)\n",
        "        self.params['b2'] = cp.zeros(hidden_size)\n",
        "        self.params['W3'] = weight_init_std * \\\n",
        "                            cp.random.randn(hidden_size, output_size)\n",
        "        self.params['b3'] = cp.zeros(output_size)\n",
        "\n",
        "        # 계층 생성\n",
        "        self.layers = OrderedDict()\n",
        "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
        "                                           conv_param['stride'], conv_param['pad'])\n",
        "        self.layers['Relu1'] = Relu()\n",
        "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
        "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
        "        self.layers['Relu2'] = Relu()\n",
        "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
        "\n",
        "        self.last_layer = SoftmaxWithLoss()\n",
        "\n",
        "    def predict(self, x):\n",
        "        for layer in self.layers.values():\n",
        "            x = layer.forward(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def loss(self, x, t):\n",
        "        \"\"\"손실 함수를 구한다.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "        \"\"\"\n",
        "        y = self.predict(x)\n",
        "        return self.last_layer.forward(y, t)\n",
        "\n",
        "    def accuracy(self, x, t, batch_size=100):\n",
        "        if t.ndim != 1 : t = cp.argmax(t, axis=1)\n",
        "        \n",
        "        acc = 0.0\n",
        "        \n",
        "        for i in range(int(x.shape[0] / batch_size)):\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\n",
        "            y = self.predict(tx)\n",
        "            y = cp.argmax(y, axis=1)\n",
        "            acc += cp.sum(y == tt) \n",
        "        \n",
        "        return acc / x.shape[0]\n",
        "\n",
        "    def numerical_gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다（수치미분）.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        loss_w = lambda w: self.loss(x, t)\n",
        "\n",
        "        grads = {}\n",
        "        for idx in (1, 2, 3):\n",
        "            grads['W' + str(idx)] = numerical_gradient(loss_w, self.params['W' + str(idx)])\n",
        "            grads['b' + str(idx)] = numerical_gradient(loss_w, self.params['b' + str(idx)])\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def gradient(self, x, t):\n",
        "        \"\"\"기울기를 구한다(오차역전파법).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : 입력 데이터\n",
        "        t : 정답 레이블\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        각 층의 기울기를 담은 사전(dictionary) 변수\n",
        "            grads['W1']、grads['W2']、... 각 층의 가중치\n",
        "            grads['b1']、grads['b2']、... 각 층의 편향\n",
        "        \"\"\"\n",
        "        # forward\n",
        "        self.loss(x, t)\n",
        "\n",
        "        # backward\n",
        "        dout = 1\n",
        "        dout = self.last_layer.backward(dout)\n",
        "\n",
        "        layers = list(self.layers.values())\n",
        "        layers.reverse()\n",
        "        for layer in layers:\n",
        "            dout = layer.backward(dout)\n",
        "\n",
        "        # 결과 저장\n",
        "        grads = {}\n",
        "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\n",
        "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
        "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
        "\n",
        "        return grads\n",
        "        \n",
        "    def save_params(self, file_name=\"params.pkl\"):\n",
        "        params = {}\n",
        "        for key, val in self.params.items():\n",
        "            params[key] = val\n",
        "        with open(file_name, 'wb') as f:\n",
        "            pickle.dump(params, f)\n",
        "\n",
        "    def load_params(self, file_name=\"params.pkl\"):\n",
        "        with open(file_name, 'rb') as f:\n",
        "            params = pickle.load(f)\n",
        "        for key, val in params.items():\n",
        "            self.params[key] = val\n",
        "\n",
        "        for i, key in enumerate(['Conv1', 'Affine1', 'Affine2']):\n",
        "            self.layers[key].W = self.params['W' + str(i+1)]\n",
        "            self.layers[key].b = self.params['b' + str(i+1)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8T4cJZqXTKsv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Step 7 - execute code"
      ]
    },
    {
      "metadata": {
        "id": "4lMbW7VjgHsL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1889
        },
        "outputId": "4fa8a211-7adf-4345-b02a-630f6a34b03d"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train=cp.expand_dims(x_train,axis=1)\n",
        "x_test=cp.expand_dims(x_test,axis=1)\n",
        "\n",
        "# cp 변환\n",
        "x_train = cp.array(x_train)\n",
        "x_test = cp.array(x_test)\n",
        "t_train = cp.array(t_train)\n",
        "t_test = cp.array(t_test)\n",
        "\n",
        "\n",
        "# 시간이 오래 걸릴 경우 데이터를 줄인다.\n",
        "#x_train, t_train = x_train[:5000], t_train[:5000]\n",
        "#x_test, t_test = x_test[:1000], t_test[:1000]\n",
        "\n",
        "max_epochs = 20\n",
        "\n",
        "network = SimpleConvNet(input_dim=(1,28,28), \n",
        "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
        "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
        "                        \n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
        "                  epochs=max_epochs, mini_batch_size=100,\n",
        "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
        "                  evaluate_sample_num_per_epoch=1000)\n",
        "trainer.train()\n",
        "\n",
        "# 매개변수 보존\n",
        "network.save_params(\"params.pkl\")\n",
        "print(\"Saved Network Parameters!\")\n",
        "\n",
        "# 그래프 그리기\n",
        "markers = {'train': 'o', 'test': 's'}\n",
        "x = np.arange(max_epochs)\n",
        "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
        "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
        "plt.xlabel(\"epochs\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim(0, 1.0)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== epoch:1, train acc:0.269, test acc:0.241 ===\n",
            "=== epoch:2, train acc:0.908, test acc:0.878 ===\n",
            "=== epoch:3, train acc:0.927, test acc:0.894 ===\n",
            "=== epoch:4, train acc:0.932, test acc:0.893 ===\n",
            "=== epoch:5, train acc:0.933, test acc:0.895 ===\n",
            "=== epoch:6, train acc:0.954, test acc:0.899 ===\n",
            "=== epoch:7, train acc:0.946, test acc:0.903 ===\n",
            "=== epoch:8, train acc:0.96, test acc:0.901 ===\n",
            "=== epoch:9, train acc:0.959, test acc:0.894 ===\n",
            "=== epoch:10, train acc:0.962, test acc:0.899 ===\n",
            "=== epoch:11, train acc:0.968, test acc:0.897 ===\n",
            "=== epoch:12, train acc:0.971, test acc:0.907 ===\n",
            "=== epoch:13, train acc:0.972, test acc:0.898 ===\n",
            "=== epoch:14, train acc:0.97, test acc:0.884 ===\n",
            "=== epoch:15, train acc:0.971, test acc:0.897 ===\n",
            "=== epoch:16, train acc:0.971, test acc:0.899 ===\n",
            "=== epoch:17, train acc:0.978, test acc:0.908 ===\n",
            "=== epoch:18, train acc:0.975, test acc:0.896 ===\n",
            "=== epoch:19, train acc:0.981, test acc:0.898 ===\n",
            "=== epoch:20, train acc:0.986, test acc:0.895 ===\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9024\n",
            "Saved Network Parameters!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmcXGWd7/HPr6r3Jd1JZ+0OkIgxEFAJRMQBFFcIOixzvYoOcxlc4gwyo9eBAS4ziHi94uSOM5d7EWUcxg0VRhBQo2ERcEZESEhYEogJa3pJ0unu6qS7q3qpeu4f51RRXV3VXb2cqk7X9/16ndfZ6/z6dNX5nfOc8zzHnHOIiIgAhIodgIiIzB5KCiIikqKkICIiKUoKIiKSoqQgIiIpSgoiIpISWFIws9vM7ICZPZdjvpnZTWa2x8yeMbOTg4pFRETyE+SVwneAc8aZvx5Y5XcbgFsCjEVERPIQWFJwzv0G6B5nkfOB7znP40CjmS0LKh4REZlYWRG33QLsTRtv9ad1ZC5oZhvwriaora095bjjjitIgCIic8XWrVsPOucWTbRcMZOCZZmWtc0N59ytwK0A69atc1u2bAkyLhGRGXXPtjY2bt5FeyRKc2M1V569mgvWthQ0BjN7NZ/lipkUWoGj0saXA+1FikVE5rBiHpTv2dbGNXc/S3Q4DkBbJMo1dz8LkDOGeMJxODZMb9TrIgNe/8SWBlYurA003mImhfuAy83sx8DbgV7n3JiiIxGZvtlwplosUzkoz5SReIIbf/lCattJ0eE4f3fPczzxSje9A2kH/+gQvQPDHB4cIVtbpV8+/4QjNymY2Y+As4CFZtYKfBEoB3DOfRPYBJwL7AEGgEuDikWkmIp9QC7mQTE9hkLtg9hwnM7Dgxw4PEjn4UG+eN+OrAfl6+59jsODI1SXh6mpCFNdHqYqOeyPp/rlYUIhIzYcp7t/iO7+Ibr6h+juH6Srbyg1Lb3r6h+iNzqcM86+wRHu37GPedXlNFSXs7CugmMX1dJQXU5DTYXXry6nsbqchhqvv7ShKpB9ls6OtKazdU9BjiSZB2SA6vIwX/2TN+d9UIwnHLHhONHhOPGEYyThSPj9eMKRcI6RuN/3p8XT5v31j7bR1T805nMX1Vdy+6feTnk4REVZiAq/X+kPh0LZbvtNLJFwxN3rMdz7dBs3/GwnseFEapmq8hDXfvB41p+Y/wOHiYSjq3+ITv9gnzzod/YNcuBQjM4+b/xwbGRKcU+kIhxiKJ7IOi8cMubXVNBUW8GC2goW1HnD82sq+M5jr2RNDi2NVfz26vcGEms2ZrbVObduwuWUFGSuK8aZevKM8vybf0vn4cEx82srw5z31haiQyNEh+MMDMWJ+f3ocJxoWn9wJPuBKGhlIfOShZ8kysMhnJ94siWguJ8MCnlIqakIs7i+kkX1lSyur2KRP5zq6ir51He3sO9QbMy6yxqquPfy04kNJRgYHvH2ub/fk/+H1P9kKE5sJM68qnLvoF+blgBqK5hXVZ4zic7EicFMyDcpFPOegkjgplt0Ek+41MF5YGgko+jAH+7zihG6B4a9ft8Q/UPxcT+3fzDOg8/vTxVdJIstls4rp6oiTE2y6MIvvkguUxYKEQ5ByIyysHl9f1o4vW9GKARloRCX3b6Vg31jrxQW1Fbw5fNPZCgeZ2gkwdBIgsGRBEPxRGp8KH08niBsRjiU0ZkRDnv9spARCo3u/69NL+TcD18+/4QJ/wcpZiyoqfATgHfQr62c+BB29frjsh6UrzrnOBbXB18ck/yeHSn3dHSlILNWPOF4paufHe2H2Nl+iJcP9gH4B6MQYUs/EKYdoNIOjj94/FX6BscWJ9RUhHn/miWjzsjT+8kzxaEJztIry0LeGWNdBQtqK1NFBk113hnkxs276M5SdNPSWM1vr37PzOyoCRT7TPX0G39NWyQ6Znqh98GRclAOiq4UZMY459h9oI8d7b001VayrKGKpQ1V1FeV57V+Pj/I2HCcP+w/nEoAO9p7eWHfYQb8M+7ysLGiqZZwyLKXqadNS81zLudBfWAozva9kdTNxJqKMI015a/faCwPU11R5vdDVFeUUVMeThUXJLuaijBmucveq8vDWQ/IV569Oq99NxOKfaZ65dmrZ8U+KLUkMFW6UpCs+gZH+O2egzyyq5NHdx2gvXdsmWxtRZilDVUsa6j2+1UsmVeVShrLGqp5dNcB/sdPnxt1QKgqD/GJM1ayoKbCTwCH2NPZRzzhfRfrKstYs2wea5rncUKz11+1uJ6Kssm3yqKz1NlB+6D4dKNZJsU5xx/29/HIrgM8squTLa92Mxx31FWWcfobmzhr9WJOPno+vdFh9h2Ksa83SkdvjH29MTp6Y+w/5HWJSX6dFtdXckLzPE5obkglgaPm10z5yZdMxS46EZktVHwkKbnO0g7Hhvntni4e/YOXCDr8q4HjltbziTNWctabFnPKMfPzPkMfiSc42DdER2+Ufb0x9h2K8aWf7cy5/JPXvo9F9ZUz8jfmUuyiE5Ejja4UCmA2VbEH71HDY5pqeLVrgJGEo76yjNPfuJCzVi/iXasXsayhesa2PxuKb0REVwqzRr6PRI7EExyKjfjtnAylqr33RofpHRgmEh2mLzYy6hlx74ZqgngC4okEcUdqWiIBI4kEz7b1MhwfnfhHEo5Xuwb49DvfwLvetIhTjplPeTiYVtRnw01GEcmfkkLANm7elbWK/VV3PcO//MdLRAaGORT12joZT01FmLrKMsrDodTz5yHz+xnPhSefUa8pKxuTEJLiCcdV5wTfBLmKb0SOLEoKATgcG+aJl7t57MWurEUnAIMjCZbOq2L1knrmVZfTWOO3c+L3ve719k+m8uQN5C6+aW6cuSKiiehxQJEjh5LCDIgOxdn6ag+PvXiQx17s4tm2XuIJR6Xfjky2ZgpaGqv51z9/W+CxqfhGSt7GVdB/YOz02sVw5e7CxzPLKSlMwdBIgqdbIzy2p4vHXjzIttciDMUTlIWMk45q5LNnHcs7jl3I2qMb+dVz+4p6UC568Y1+kFJs2b5/400vcUoKeToUG+aHv3+Nx17s4smXu4kOxzGDE5sbuPT0Fbzj2CbetmLBmLZYin5QpsjFN/pBzoyhfji8D2oXQlVDsaM5coyMbWJExqekkKc7ntjLjb98gVWL6/jo247iHcc28faVC2isqRh/xY2ruKD/ABcAVAEx4F7gwQKdKRfjTN05GInB4OHxl+t4GsKVUFYB4Yq04Uooq4Rxmo/IW7GvVPLdvnPQdwB6XoGel6H75dHD6Z9R1QANR0PjUdB4NDQclTZ8NNQsGL3vjpR9MBVDA9C7FyJ7ofc1iLzmD+/1hg/vG3/9n30OFq6GRath0XEwr3lmvnfpir3/J0lJIU97ewaoryrjgS+8a3IrFvtMeTLbT8RhqM87mA8m+4f8fnrnT0ste3jssok82rT/1jvHnx8q95JDuMLrl1dD7SKoW/J6V58cXgx1S7354bSv9VT2/8ggRHsgGoFY5PXhwcMQCvsxpSewtESWjDXZH2/7m6/1DvzJBDDcn7aAwbwWWLAS3vQBmL8S6pdBf+frB8Hul+Hl33j/h3TlNX6i8BPHbP4O9rzinc3HB9P6gxAfyugPwnAMDrV5B/vkPhg4OPozQ2XQsNz7+499j7cPHvlq7th23gfR7tfHK+ph0Zu8BLFo9esJo/EYCE3wsIdz3vc+M+7x/v74yOjv6ywwu6KZxdojMVpm+omd3Q+AhbwDTagMzO+HQmnjyXkhr8v2Y8n8UcWHXh8ez7+8Z/RBPfPgkkt5DVTWj+7mr/D6FXWjp2+6IvfnfPT27PHnOigMDXgHxc5d8PKjEOvN8qEGNU1Qv9RLFOO55zLvYB/t8Q/+/vBI9ifGZtyT3/b22/yVsPKdXgKYv9LrNxwF5Xk06+ycF3P6gTJ5lhx5DdomqOi5+0FY9paJ99VkOOcdvPfv8Lrx/J+3Tu6zy6peT3jL3uoPH+Mlv4ajvP97KDx6nfGSwlUvQ/9B6HzB73Z53Z6HYPvtaduthqZjc/wGh1//3jLJysBfbvJOfipqvN9VqqvOmFYNFbVwwoVw9GmT28YkKSnkqT0SZVm+r8I7vA9at0Drk+Mvd/uHpx/YdFTP935IlfVQOW/sgX7MtDrvTGoyZzbjJYXjPzS9+Idj3tnW4f3Ql+wOQN8+v79//PVfehSqG739sOANrw9X+f3qxtHDlfOynwmOl9R+/vnc2/8fHROffU7EzCsuqlkAzSdlX+b6ce5B3P5fvH7dElj6Fi9BLH2zNzx/5cTxxXph/044sMPv7/T6g9kSdhYX3JJ2dZXl6itcMfpKrHr+5It3ahfnLr4B7z5N7Rmw4ozR86M90PkHOOgniq493vTx4s1WFHrvZblje/e1MDzgnfAMD8Bw1O/70/q7vCvI4ag3vuREJYXZoqM3ytqjG8fOGI55ZeNtfhJo3eKdqYF3hj+eTz3kFdkkRsD5/UQibTw5L/F6P+cXMkdRxlfHucF88V1T3yH5mugHOR3lVX4RydG5lxnvgPiFCc5iZ8J4SWG6CWEmXPJz2Pcs7HsGOp6BF3/tfffAOwFYeqKXIJa+GRau8q5E0hNA8rsOUNkAS9bAmz/s9RefAIuPh68dk3v7J3082L8Ppl5uXz0fjn67103HeEnhXX87vc8OgJJCHqJDcXoGhmluqIKuF6Ftq58AnoR9z0HCf/9qw9GwfB2c9pew/G3ej+krS3J/8PIJmyE58s3CG2mSZuWZXpc0HIPO570EkUwW234w+n5HqBwWvsk7Y138CVhygtfNa5n5m7RScEoKeWjvjXJaaCefevxy+I1/U6q8FlpOhj+63EsALeu8m56ZgjxTzkext19sxf77i739ycZQXgXNa70uKZGA7pe84pPGo6Hpjd4VaRDbn4uOsL9fraTm4T92d7L9u1dwefnPsA/+o5cEFh8/9oaWiMgspVZSZ1BHJEazHSRet5SydZcWOxwRkcDMgjtds19bJEqLdRFqWF7sUEREAqWkkIeO3ijLQ92EGo8qdigiIoFSUshDR88AS+iCBjX/LCJzm5JCHgYi+yhnxKvoJSIyhykpTMA5R+hQqzeiewoiMscpKUygZ2CYhfFOb2Seio9EZG5TUphAeyRKs3V5I7pSEJE5TklhAsmkEC+r8dpCERGZw5QUJtDRG2OZdeHUrouIlAAlhQm0R6Isty7CqqMgIiVASWEC7b0xWkJdmOooiEgJUFKYwIGeXpqIqI6CiJSEQJOCmZ1jZrvMbI+ZXZ1l/tFm9rCZbTOzZ8zs3CDjmYpEpM0b0JNHIlICAksKZhYGbgbWA2uAj5nZmozF/g640zm3FrgI+EZQ8UzFSDxBeV8yKaj4SETmviCvFE4F9jjnXnLODQE/Bs7PWMYB8/zhBqA9wHgm7cDhQZaSrKOg4iMRmfuCTAotQNoLXGn1p6W7HrjYzFqBTcBfZfsgM9tgZlvMbEtnZ2cQsWY1quLavOaCbVdEpFiCTArZHurPfM3bx4DvOOeWA+cC3zezMTE55251zq1zzq1btGhRAKFm194bo9m6GKlqgvLqgm1XRKRYgkwKrUB6mctyxhYPfRK4E8A59zugClgYYEyTkrxSsEbdZBaR0hBkUngSWGVmK82sAu9G8n0Zy7wGvBfAzI7HSwqFKx+aQEckyvKwKq6JSOkILCk450aAy4HNwPN4TxntMLMbzOw8f7G/AT5tZk8DPwL+3DmXWcRUNG09UZrp0uOoIlIyyoL8cOfcJrwbyOnTrksb3gmcHmQM03EocpAaomoyW0RKhmo0j8P16uU6IlJalBRyiA7FqRvc742ojoKIlAglhRzae6O02EFvRLWZRaREKCnk0BHx3qOQsDKoW1LscERECkJJIYdkHYVE3VIIhYsdjohIQSgp5NDe6yWFkOooiEgJUVLIoT0SZXmoW0lBREqKkkIO+yL9LKFLN5lFpKQoKeQQ7emgjLjqKIhISVFSyMI5R/iQ/3KdeUoKIlI6lBSyiAwMsyDut8unKwURKSFKClkknzwClBREpKQoKWTRHonRYgeJl9dCVUOxwxERKRglhSw6eqMss27cvOVg2V4gJyIyNykpZNEW8do90st1RKTUKClk0RGJ0RLqxlRHQURKjJJCFgd7IiygV01mi0jJUVLIYqQn+XIdXSmISGlRUsgwEk9QMdDhjehxVBEpMUoKGQ4cHmQpyZfrKCmISGlRUsjQ0RulGb/i2jwVH4lIaVFSyNDmv3FtpHoRlFUWOxwRkYJSUsjQ4ddR0OOoIlKKlBQytEeitIS6Cc/X46giUnqUFDIk382sOgoiUoqUFDIc6umkmphuMotISVJSyJR8uY4eRxWREqSkkCY6FKcupoprIlK6lBTSJJvMBpQURKQkKSmkSb5cJxEqh9rFxQ5HRKTglBTSJF/DmahbBiHtGhEpPTrypWmPRFlmXYT0ch0RKVFKCmk6IjGOCnURatT9BBEpTUoKaToifSymWzeZRaRkBZoUzOwcM9tlZnvM7Oocy3zEzHaa2Q4z+2GQ8UxksKedMAklBREpWWVBfbCZhYGbgfcDrcCTZnafc25n2jKrgGuA051zPWZWtEd+nHOEDrVBGJinpCAipSnIK4VTgT3OuZecc0PAj4HzM5b5NHCzc64HwDl3IMB4xtUbHaYp3umN6EpBREpUkEmhBdibNt7qT0v3JuBNZvZbM3vczM7J9kFmtsHMtpjZls7OzkCCbYtEabbkG9fU7pGIlKYgk4JlmeYyxsuAVcBZwMeAb5tZ45iVnLvVObfOObdu0aJFMx4oeE8eLbNu4uX1UNUQyDZERGa7vJKCmd1lZh80s8kkkVYg/YH/5UB7lmXudc4NO+deBnbhJYmCa+/1Xq7j1DqqiJSwfA/ytwAfB3ab2Y1mdlwe6zwJrDKzlWZWAVwE3JexzD3AuwHMbCFecdJLecY0o9ojMVpCXXq5joiUtLySgnPuQefcnwInA68AD5jZY2Z2qZmV51hnBLgc2Aw8D9zpnNthZjeY2Xn+YpuBLjPbCTwMXOmc65renzQ17ZEoLdaN6SaziJSwvB9JNbMm4GLgz4BtwO3AGcAlePcExnDObQI2ZUy7Lm3YAV/wu6LqivTQyCE9eSQiJS2vpGBmdwPHAd8H/tg55790gDvMbEtQwRVSokcv1xERyfdK4f85536dbYZzbt0MxlMU8YSjrL8dylFSEJGSlu+N5uPTHxU1s/lmdllAMRXcgcMxluLXUdDTRyJSwvJNCp92zkWSI34N5E8HE1LhtUdiNNOFw2Bec7HDEREpmnyTQsjMUpXR/HaNKoIJqfDaI97LdUZqFkFZZbHDEREpmnzvKWwG7jSzb+LVSv4L4FeBRVVgHb1RjreDhHQ/QURKXL5J4SrgM8Bf4jVfcT/w7aCCKrT2SIz3hboJz39bsUMRESmqvJKCcy6BV6v5lmDDKY62ngGvMTw1mS0iJS7fegqrgK8Ca4Cq5HTn3BsCiqug+iIHqGJIj6OKSMnL90bzv+FdJYzgtVX0PbyKbHNDb6vXV1IQkRKXb1Kods49BJhz7lXn3PXAe4ILq3Biw3HqYvu8Eb1HQURKXL43mmN+s9m7zexyoA0o2qszZ1J7JMoy89vga1ALqSJS2vK9Uvg8UAP8NXAKXsN4lwQVVCF19MZoti4SoQqoWVjscEREimrCKwW/otpHnHNXAn3ApYFHVUBtfsW1eH0zoVCQL6ITEZn9JjwKOufiwCnpNZrnko6Id6UQblTRkYhIvvcUtgH3mtm/A/3Jic65uwOJqoDaI1GWh7oINZ5S7FBERIou36SwAOhi9BNHDjjik8K+SB+L6NHjqCIi5F+jeU7dR0g31NNGmISazBYRIf8azf+Gd2UwinPuEzMeUQE55wgdbvPurOhxVBGRvIuPfp42XAVcCLTPfDiF1Rsdpmmk02sEXMVHIiJ5Fx/dlT5uZj8CHgwkogJqj8S8hvBAtZlFRMi/8lqmVcDRMxlIMSRrM49UzIPK+mKHIyJSdPneUzjM6HsK+/DesXBE6+j1Kq7pJrOIiCff4qM5eRrdFolxSqiL8Pzjih2KiMiskFfxkZldaGYNaeONZnZBcGEVRkdvlBbrwnSTWUQEyP+ewhedc73JEedcBPhiMCEVTld3Dw306SaziIgv36SQbbl8H2edteKR5Mt1VEdBRATyTwpbzOzrZnasmb3BzP4J2BpkYEGLJxwV/X5VC91oFhEB8k8KfwUMAXcAdwJR4LNBBVUInYcHWUqyjoLuKYiIQP5PH/UDVwccS0El36PgMGxec7HDERGZFfJ9+ugBM2tMG59vZpuDCyt4Hb1RmjnISM1iCJcXOxwRkVkh3+Kjhf4TRwA453o4wt/RnKzNbHq5johISr5JIWFmqWYtzGwFWVpNPZK0R2IsD3UTnq+kICKSlO9jpdcC/2lmj/rj7wQ2BBNSYbT3DHhXCnrySEQkJd8bzb8ys3V4iWA7cC/eE0hHrP7IASoZUh0FEZE0+d5o/hTwEPA3fvd94Po81jvHzHaZ2R4zy/n0kpl92Mycn3gKo3ev11dtZhGRlHzvKXwOeBvwqnPu3cBaoHO8FcwsDNwMrAfWAB8zszVZlqsH/hr4/STinpbYcJza2D5vRHUURERS8k0KMedcDMDMKp1zLwCrJ1jnVGCPc+4l59wQ8GPg/CzLfRn4ByCWZyzT1tEb85rMBhUfiYikyTcptPr1FO4BHjCze5n4dZwtwN70z/CnpZjZWuAo51z66z7HMLMNZrbFzLZ0do57gZKXDr/iWjxcCTVN0/48EZG5It8bzRf6g9eb2cNAA/CrCVazbB+VmmkWAv4J+PM8tn8rcCvAunXrpv0obLI2c6KuhbBlC1NEpDRNuqVT59yjEy8FeFcG6WUzyxl9dVEPnAg8Yt6BeSlwn5md55zbMtm4JqM9EuNMO0hovu4niIikm+o7mvPxJLDKzFaaWQVwEXBfcqZzrtc5t9A5t8I5twJ4HAg8IYD/cp1QN2HVZhYRGSWwpOCcGwEuBzYDzwN3Oud2mNkNZnZeUNvNx76ewyyiR08eiYhkCPRFOc65TcCmjGnX5Vj2rCBjSTccaSOE03sUREQyBFl8NCs55wgdavNGdKUgIjJKySWFQ9ERFowc8EaUFERERim5pOA9jtrtjaj4SERklJJLCh29UZrtICOVjVBZV+xwRERmlZJLCu3J13DqKkFEZIzSSwq9MZpDXZTp5ToiImOUXlKIRFluXZhuMouIjFFySaGnu5t6+vXkkYhIFiWXFOKRVm9ATWaLiIxRUkkhnnBU9vsV13SjWURkjJJKCp2HB1lM8uU6Kj4SEclUUkmh3a+j4CwE9cuKHY6IyKxTWkkhEqXFuhipWQLhQNsCFBE5IpVUUuiIxFhGF6b3KIiIZFVSSaEtEmV5qItwo+4niIhkU1JJYV+kn6XWrYprIiI5lFRS6O/ZTwXDqqMgIpJDSSUFO5SsuKY6CiIi2ZRMUogNx6mO7vNGVHwkIpJVSSSFe7a18a6ND9NiXsW1Ta+GixyRiMjsNOeTwj3b2rjm7mfZf2iQZdZF1FXwN7/Yyz3b2oodmojIrDPnk8LGzbuIDscBaLaDtLsmosMJNm7eVeTIRERmnzmfFNoj0dRwi3XR7prGTBcREc+cTwrNjdWp4WXWRbtbOGa6iIh45nxSuPLs1VSXhylnhEX00sECqsvDXHn26mKHJiIy65hzrtgxTMq6devcli1b8l9h4yroPzB2eu1iuHL3zAUmIjKLmdlW59y6iZab81cKWRPCeNNFRErY3E8KIiKSNyUFERFJUVIQEZEUJQUREUmZ+0mhdvHkpouIlLC5/6JiPXYqIpK3uX+lICIieQs0KZjZOWa2y8z2mNnVWeZ/wcx2mtkzZvaQmR0TZDwiIjK+wJKCmYWBm4H1wBrgY2a2JmOxbcA659xbgJ8A/xBUPCIiMrEgrxROBfY4515yzg0BPwbOT1/AOfewc27AH30c0CvRRESKKMik0ALsTRtv9afl8kngl9lmmNkGM9tiZls6OztnMEQREUkXZFKwLNOytr5nZhcD64CN2eY75251zq1zzq1btGjRDIYoIiLpgnwktRU4Km18OdCeuZCZvQ+4FniXc24wwHhERGQCQV4pPAmsMrOVZlYBXATcl76Ama0FvgWc55xTs6UiIkUWWFJwzo0AlwObgeeBO51zO8zsBjM7z19sI1AH/LuZbTez+3J8nIiIFECgNZqdc5uATRnTrksbfl+Q2xcRkcmZ+81ciIgAw8PDtLa2EovFih1KoKqqqli+fDnl5eVTWl9JQURKQmtrK/X19axYsQKzbA9HHvmcc3R1ddHa2srKlSun9Blq+0hESkIsFqOpqWnOJgQAM6OpqWlaV0NKCiJSMuZyQkia7t+opCAiIilKCiIiWdyzrY3Tb/w1K6/+Baff+Gvu2dY2rc+LRCJ84xvfmPR65557LpFIZFrbngwlBRGRDPdsa+Oau5+lLRLFAW2RKNfc/ey0EkOupBCPx8ddb9OmTTQ2Nk55u5Olp49EpOR86Wc72Nl+KOf8ba9FGIonRk2LDsf52588w4+eeC3rOmua5/HFPz4h52deffXVvPjii5x00kmUl5dTV1fHsmXL2L59Ozt37uSCCy5g7969xGIxPve5z7FhwwYAVqxYwZYtW+jr62P9+vWcccYZPPbYY7S0tHDvvfdSXV09hT2Qm64UREQyZCaEiabn48Ybb+TYY49l+/btbNy4kSeeeIKvfOUr7Ny5E4DbbruNrVu3smXLFm666Sa6urrGfMbu3bv57Gc/y44dO2hsbOSuu+6acjy56EpBRErOeGf0AKff+GvaItEx01saq7njM++YkRhOPfXUUXUJbrrpJn76058CsHfvXnbv3k1TU9OodVauXMlJJ50EwCmnnMIrr7wyI7Gk05WCiEiGK89eTXV5eNS06vIwV569esa2UVtbmxp+5JFHePDBB/nd737H008/zdq1a7PWNaisrEwNh8NhRkZGZiyeJF0piIhkuGCt9z6wjZt30R6J0txYzZVnr05Nn4r6+noOHz6cdV5vby/z58+npqaGF154gcf6of/xAAAKXElEQVQff3zK25kuJQURkSwuWNsyrSSQqampidNPP50TTzyR6upqlixZkpp3zjnn8M1vfpO3vOUtrF69mtNOO23GtjtZ5lzWl6HNWuvWrXNbtmwpdhgicoR5/vnnOf7444sdRkFk+1vNbKtzbt1E6+qegoiIpCgpiIhIipKCiIikKCmIiEiKkoKIiKQoKYiISIrqKYiIZNq4CvoPjJ1euxiu3D2lj4xEIvzwhz/ksssum/S6//zP/8yGDRuoqamZ0rYnQ1cKIiKZsiWE8abnYarvUwAvKQwMDEx525OhKwURKT2/vBr2PTu1df/tg9mnL30zrL8x52rpTWe///3vZ/Hixdx5550MDg5y4YUX8qUvfYn+/n4+8pGP0NraSjwe5+///u/Zv38/7e3tvPvd72bhwoU8/PDDU4s7T0oKIiIFcOONN/Lcc8+xfft27r//fn7yk5/wxBNP4JzjvPPO4ze/+Q2dnZ00Nzfzi1/8AvDaRGpoaODrX/86Dz/8MAsXLgw8TiUFESk945zRA3B9Q+55l/5i2pu///77uf/++1m7di0AfX197N69mzPPPJMrrriCq666ig996EOceeaZ097WZCkpiIgUmHOOa665hs985jNj5m3dupVNmzZxzTXX8IEPfIDrrruuoLHpRrOISKbaxZObnof0prPPPvtsbrvtNvr6+gBoa2vjwIEDtLe3U1NTw8UXX8wVV1zBU089NWbdoOlKQUQk0xQfOx1PetPZ69ev5+Mf/zjveIf3Fre6ujp+8IMfsGfPHq688kpCoRDl5eXccsstAGzYsIH169ezbNmywG80q+lsESkJajpbTWeLiMgkKSmIiEiKkoKIlIwjrbh8Kqb7NyopiEhJqKqqoqura04nBuccXV1dVFVVTfkz9PSRiJSE5cuX09raSmdnZ7FDCVRVVRXLly+f8vpKCiJSEsrLy1m5cmWxw5j1Ai0+MrNzzGyXme0xs6uzzK80szv8+b83sxVBxiMiIuMLLCmYWRi4GVgPrAE+ZmZrMhb7JNDjnHsj8E/A14KKR0REJhbklcKpwB7n3EvOuSHgx8D5GcucD3zXH/4J8F4zswBjEhGRcQR5T6EF2Js23gq8PdcyzrkRM+sFmoCD6QuZ2QZggz/aZ2a7phjTwszPnmUU3/Qovumb7TEqvqk7Jp+FgkwK2c74M58Fy2cZnHO3ArdOOyCzLflU8y4WxTc9im/6ZnuMii94QRYftQJHpY0vB9pzLWNmZUAD0B1gTCIiMo4gk8KTwCozW2lmFcBFwH0Zy9wHXOIPfxj4tZvLNUtERGa5wIqP/HsElwObgTBwm3Nuh5ndAGxxzt0H/CvwfTPbg3eFcFFQ8fimXQQVMMU3PYpv+mZ7jIovYEdc09kiIhIctX0kIiIpSgoiIpIyJ5PCbG5ew8yOMrOHzex5M9thZp/LssxZZtZrZtv9rqBv7jazV8zsWX/bY15zZ56b/P33jJmdXMDYVqftl+1mdsjMPp+xTMH3n5ndZmYHzOy5tGkLzOwBM9vt9+fnWPcSf5ndZnZJtmUCiG2jmb3g//9+amaNOdYd97sQcIzXm1lb2v/x3Bzrjvt7DzC+O9Jie8XMtudYtyD7cMY45+ZUh3dT+0XgDUAF8DSwJmOZy4Bv+sMXAXcUML5lwMn+cD3whyzxnQX8vIj78BVg4TjzzwV+iVfP5DTg90X8X+8Djin2/gPeCZwMPJc27R+Aq/3hq4GvZVlvAfCS35/vD88vQGwfAMr84a9liy2f70LAMV4PXJHHd2Dc33tQ8WXM/0fgumLuw5nq5uKVwqxuXsM51+Gce8ofPgw8j1ez+0hyPvA953kcaDSzZUWI473Ai865V4uw7VGcc79hbB2b9O/Zd4ELsqx6NvCAc67bOdcDPACcE3Rszrn7nXMj/ujjePWIiibH/stHPr/3aRsvPv/Y8RHgRzO93WKYi0khW/MamQfdUc1rAMnmNQrKL7ZaC/w+y+x3mNnTZvZLMzuhoIF5tcrvN7OtfhMjmfLZx4VwEbl/iMXcf0lLnHMd4J0MAIuzLDMb9uUn8K78spnouxC0y/0irttyFL/Nhv13JrDfObc7x/xi78NJmYtJYcaa1wiSmdUBdwGfd84dypj9FF6RyFuB/wvcU8jYgNOdcyfjtXD7WTN7Z8b82bD/KoDzgH/PMrvY+28yirovzexaYAS4PcciE30XgnQLcCxwEtCBV0STqejfReBjjH+VUMx9OGlzMSnM+uY1zKwcLyHc7py7O3O+c+6Qc67PH94ElJvZwkLF55xr9/sHgJ/iXaKny2cfB2098JRzbn/mjGLvvzT7k8Vqfv9AlmWKti/9m9ofAv7U+YXfmfL4LgTGObffORd3ziWAf8mx7aJ+F/3jx58Ad+Rappj7cCrmYlKY1c1r+OWP/wo875z7eo5llibvcZjZqXj/p64CxVdrZvXJYbwbks9lLHYf8N/8p5BOA3qTxSQFlPPsrJj7L0P69+wS4N4sy2wGPmBm8/3ikQ/40wJlZucAVwHnOecGciyTz3chyBjT71NdmGPb+fzeg/Q+4AXnXGu2mcXeh1NS7DvdQXR4T8f8Ae+phGv9aTfg/QAAqvCKHfYATwBvKGBsZ+Bd3j4DbPe7c4G/AP7CX+ZyYAfekxSPA39UwPje4G/3aT+G5P5Lj8/wXqD0IvAssK7A/98avIN8Q9q0ou4/vATVAQzjnb1+Eu8+1UPAbr+/wF92HfDttHU/4X8X9wCXFii2PXhl8cnvYPJpvGZg03jfhQLuv+/7369n8A70yzJj9MfH/N4LEZ8//TvJ713askXZhzPVqZkLERFJmYvFRyIiMkVKCiIikqKkICIiKUoKIiKSoqQgIiIpSgoiAfNbbf15seMQyYeSgoiIpCgpiPjM7GIze8Jv9/5bZhY2sz4z+0cze8rMHjKzRf6yJ5nZ42nvI5jvT3+jmT3oN8b3lJkd6398nZn9xH+Hwe1pNa5vNLOd/uf87yL96SIpSgoigJkdD3wUr/Gyk4A48KdALV4bSycDjwJf9Ff5HnCVc+4teLVuk9NvB252XmN8f4RXCxa81nA/D6zBq+V6upktwGu+4QT/c/5nsH+lyMSUFEQ87wVOAZ7036D1XryDd4LXGzv7AXCGmTUAjc65R/3p3wXe6bdx0+Kc+ymAcy7mXm9X6AnnXKvzGnfbDqwADgEx4Ntm9idA1jaIRApJSUHEY8B3nXMn+d1q59z1WZYbr12Y8V7UNJg2HMd769kIXouZd+G9gOdXk4xZZMYpKYh4HgI+bGaLIfV+5WPwfiMf9pf5OPCfzrleoMfMzvSn/xnwqPPei9FqZhf4n1FpZjW5Nui/U6PBec17fx7vvQEiRVVW7ABEZgPn3E4z+zu8N2SF8FrD/CzQD5xgZlvx3tD3UX+VS4Bv+gf9l4BL/el/BnzLzG7wP+O/jrPZeuBeM6vCu8r47zP8Z4lMmlpJFRmHmfU55+qKHYdIoaj4SEREUnSlICIiKbpSEBGRFCUFERFJUVIQEZEUJQUREUlRUhARkZT/D3nNCNcqeD2xAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}